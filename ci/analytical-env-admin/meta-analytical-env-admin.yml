meta-analytical-env-admin:
  plan:
    terraform-common-config:
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.terraform_repository))
            tag: ((dataworks.terraform_version))
        params:
          TF_INPUT: false
          TF_CLI_ARGS_apply: -lock-timeout=300s
          TF_CLI_ARGS_plan: -lock-timeout=300s
          TF_VAR_costcode: ((dataworks.costcode))

    terraform-bootstrap:
      task: terraform-bootstrap
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ../previous_success/exit-if-succeeded.sh ]; then
                source ../previous_success/exit-if-succeeded.sh
              fi
              python bootstrap_terraform.py
              sed -i '/^assume_role/ d' terraform/deploy/$DEPLOY_PATH/terraform.tfvars
              cp terraform/deploy/$DEPLOY_PATH/terraform.tf ../terraform-config
              cp terraform/deploy/$DEPLOY_PATH/terraform.tfvars ../terraform-config
          dir: aws-analytical-env
        inputs:
          - name: aws-analytical-env
        outputs:
          - name: terraform-config
      params:
        AWS_REGION: eu-west-2

    terraform-apply:
      task: terraform-apply
      .: (( inject meta-analytical-env-admin.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ../../../../previous_success/exit-if-succeeded.sh ]; then
                source ../../../../previous_success/exit-if-succeeded.sh
              fi
              apk add -q --update --no-cache python3 && ln -sf python3 /usr/bin/python
              python3 -m ensurepip
              pip3 install -q --no-cache --upgrade pip setuptools boto3
              cp ../../../../terraform-config/terraform.tf .
              cp ../../../../terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              export TF_VAR_emr_al2_ami_id=$(cat ../../../../emr-al2-ami/id)
              export TF_VAR_aws_analytical_env_emr_launcher_zip="{base_path = \"../../../../emr-launcher-release\", version = \"$(cat ../../../../emr-launcher-release/version)\"}"
              export TF_VAR_manage_mysql_user_lambda_zip="{base_path = \"../../../../manage-mysql-user-release\", version = \"$(cat ../../../../manage-mysql-user-release/version)\"}"
              custom_auth_jar_version=$(cat ../../../../hive-custom-auth-release/version) || true
              export TF_VAR_hive_custom_auth_jar_path="../../../../hive-custom-auth-release/hive-custom-auth-${custom_auth_jar_version}.jar"
              terraform plan -out terraform.plan
              terraform apply -auto-approve terraform.plan
        inputs:
          - name: aws-analytical-env
          - name: terraform-config
          - name: emr-al2-ami

    terraform-plan:
      task: terraform-plan
      .: (( inject meta-analytical-env-admin.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ../../../../previous_success/exit-if-succeeded.sh ]; then
                source ../../../../previous_success/exit-if-succeeded.sh
              fi
              apk add -q --update --no-cache python3 && ln -sf python3 /usr/bin/python
              python3 -m ensurepip
              pip3 install -q --no-cache --upgrade pip setuptools boto3
              cp ../../../../terraform-config/terraform.tf .
              cp ../../../../terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              export TF_VAR_emr_al2_ami_id=$(cat ../../../../emr-al2-ami/id)
              export TF_VAR_aws_analytical_env_emr_launcher_zip="{base_path = \"../../../../emr-launcher-release\", version = \"$(cat ../../../../emr-launcher-release/version)\"}"
              export TF_VAR_manage_mysql_user_lambda_zip="{base_path = \"../../../../manage-mysql-user-release\", version = \"$(cat ../../../../manage-mysql-user-release/version)\"}"
              custom_auth_jar_version=$(cat ../../../../hive-custom-auth-release/version) || true
              export TF_VAR_hive_custom_auth_jar_path="../../../../hive-custom-auth-release/hive-custom-auth-${custom_auth_jar_version}.jar"
              terraform plan
        inputs:
          - name: aws-analytical-env
          - name: terraform-config
          - name: emr-al2-ami

    terraform-output-app:
      task: terraform-output-app
      .: (( inject meta-analytical-env-admin.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              cp ../../../../terraform-config/terraform.tf .
              cp ../../../../terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              export TF_VAR_emr_al2_ami_id=$(cat ../../../../emr-al2-ami/id)
              terraform output --json > ../../../../terraform-output-app/outputs.json
          dir: aws-analytical-env/terraform/deploy/app
        inputs:
          - name: aws-analytical-env
          - name: terraform-config
          - name: emr-al2-ami
        outputs:
          - name: terraform-output-app

    terraform-taint:
      task: terraform-taint
      .: (( inject meta-analytical-env-admin.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              cp terraform-config/terraform.tf .
              cp terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              terraform taint "module.emr.aws_emr_cluster.cluster"
        inputs:
          - name: aws-analytical-env
          - name: terraform-config

    aws-destroy-emr-cluster:
      task: aws-destroy-emr-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: UNSET
          AWS_REGION: ((dataworks.aws_region))
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -ec
            - |
              source /assume-role

              function cluster_status {
                local cluster_id=$1
                aws --output json --region="$AWS_REGION" \
                  emr describe-cluster --cluster-id $cluster_id | jq -r '.Cluster.Status.State'
              }

              cluster_id=$(aws --output json --region="$AWS_REGION" \
                                emr list-clusters --active --query "Clusters[?Name=='$CLUSTER_NAME'].[Id][0][0]" | tr -d '"')

              if [[ $cluster_id != "null" ]]; then
                echo destroying: $CLUSTER_NAME - $cluster_id
                # Terminate using AWS CLI because targetted destroys can cause
                # issues with TF (orphaned resources, etc.)
                aws --region="$AWS_REGION" emr terminate-clusters --cluster-ids $cluster_id

                i=0
                while [[ $i -le 360 ]]; do
                  status=$(cluster_status $cluster_id)
                  if [ "$status" == "TERMINATED" ]; then
                    echo status: \'$status\', cluster terminated exiting normally.
                    exit 0
                  fi

                  if [ "$status" == "TERMINATED_WITH_ERRORS" ]; then
                    echo status: \'$status\', cluster terminated with errors exiting normally.
                    exit 0
                  fi

                  i=$((i+1))
                  echo status: \'$status\', sleeping for 10 seconds.
                  sleep 10
                done

                echo Monitoring of the destruction has timed out, progressing anyway
              else
                echo No cluster with name: $CLUSTER_NAME exists, therefore not destroying. Continuing...
              fi

    rotate-mysql-master-password:
      task: rotate-mysql-master-password
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.development)):role/ci
          AWS_REGION: ((dataworks.aws_region))
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export RDS_CLUSTER_IDENTIFIER=$(jq -r .rbac_db.value.rds_cluster.cluster_identifier < ../terraform-output/outputs.json)
              export RDS_CREDENTIALS_SECRET_NAME=$(jq -r .rbac_db.value.secrets.master_credentials.name < ../terraform-output/outputs.json)
              source /assume-role
              set +x
              ./rotate_rds_master_user_password_update_secrets_manager.py $RDS_CLUSTER_IDENTIFIER $RDS_CREDENTIALS_SECRET_NAME
          dir: secrets-management
        inputs:
          - name: secrets-management
          - name: terraform-output

    rotate-mysql-client-password:
      task: rotate-client-mysql-password
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.development)):role/ci
          AWS_REGION: ((dataworks.aws_region))
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              source /assume-role

              MANAGE_USER_FUNCTION_NAME=$(jq -r ".rbac_db.value.manage_mysql_user_lambda.function_name" < ./terraform-output/outputs.json)

              CLIENTS=$(jq -r ".rbac_db.value.secrets.client_credentials | keys[]" < ./terraform-output/outputs.json)
              for client in $CLIENTS; do
                param_name=$(jq -r ".rbac_db.value.secrets.client_credentials[\"$client\"].name" < ./terraform-output/outputs.json);
                privileges=$(jq -r ".rbac_db.value.client_privileges[\"$client\"]" < ./terraform-output/outputs.json);
                jq -n --arg Username "$client" --arg Paramname "$param_name" --arg privileges "$privileges" \
                  '{mysql_user_username: $Username, mysql_user_password_secret_name: $Paramname, privileges: $privileges}' > manifest.json
                aws lambda invoke --function-name $MANAGE_USER_FUNCTION_NAME --invocation-type RequestResponse --payload file://manifest.json --cli-connect-timeout 900 --cli-read-timeout 900 output.json
                jq -eC "if .errorMessage? then error(.errorMessage) else true end" < output.json
              done || exit 1
        inputs:
          - name: terraform-output

    initialise-rbac-db:
      task: initialise-rbac-db
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.development)):role/ci
          AWS_REGION: ((dataworks.aws_region))
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        inputs:
          - name: terraform-output
        run:
          path: sh
          args:
            - -exc
            - |
              source /assume-role

              INITIALISE_DB_FUNCTION_NAME=$(jq -r ".rbac_db.value.initialise_db_lambda.function_name" < ./terraform-output/outputs.json)
              aws lambda invoke --function-name $INITIALISE_DB_FUNCTION_NAME --invocation-type RequestResponse --cli-connect-timeout 900 --cli-read-timeout 900 output.json

              jq < output.json
              jq -eC "if .errorMessage? then error(.errorMessage) else true end" < output.json

    sync_rbac_db_from_cognito:
      task: sync-rbac-db
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.development)):role/ci
          AWS_REGION: ((dataworks.aws_region))
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        inputs:
          - name: terraform-output
        run:
          path: sh
          args:
            - -exc
            - |
              source /assume-role

              SYNC_RDS_FUNCTION_NAME=$(jq -r ".emrfs_lambdas.value.rds_sync_lambda.function_name" < ./terraform-output/outputs.json)
              aws lambda invoke --function-name $SYNC_RDS_FUNCTION_NAME --invocation-type RequestResponse --cli-connect-timeout 900 --cli-read-timeout 900 output.json

              jq < output.json
              jq -eC "if .errorMessage? then error(.errorMessage) else true end" < output.json

    create_roles_and_policies_for_emr_users:
      task: create-roles-and-policies
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.development)):role/ci
          AWS_REGION: ((dataworks.aws_region))
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        inputs:
          - name: terraform-output
        run:
          path: sh
          args:
            - -exc
            - |
              source /assume-role

              POLICY_MUNGE_FUNCTION_NAME=$(jq -r ".emrfs_lambdas.value.policy_munge_lambda.function_name" < ./terraform-output/outputs.json)
              aws lambda invoke --function-name $POLICY_MUNGE_FUNCTION_NAME --invocation-type Event --cli-connect-timeout 900 output.json

              # Error if lamnbda invocation fails
              cat output.json |  jq 'if .StatusCode>=400? then "Invocation of lambda failed with Status Code: \(.StatusCode)\n"|halt_error(1) else true end'

    stop-batch-cluster:
      task: stop-batch-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +x
              for CLUSTER_ID in $(aws emr list-clusters --active | jq -r '.Clusters[] | select(.Name | test("^batch-analytical-env")) | .Id');
              do
                if [ -z "SINGLE_CLUSTER_ID" ] || [ "SINGLE_CLUSTER_ID" == "$CLUSTER_ID" ]; then
                  aws emr terminate-clusters --cluster-ids $CLUSTER_ID
                fi
              done

    stop-uc-lab-batch-cluster:
      task: stop-uc-lab-batch-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +x
              for CLUSTER_ID in $(aws emr list-clusters --active | jq -r '.Clusters[] | select(.Name | test("^uc-lab-batch-analytical-env")) | .Id');
              do
                if [ -z "SINGLE_CLUSTER_ID" ] || [ "SINGLE_CLUSTER_ID" == "$CLUSTER_ID" ]; then
                  aws emr terminate-clusters --cluster-ids $CLUSTER_ID
                fi
              done

    stop-user-batch-cluster:
      task: stop-user-batch-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +x
              for CLUSTER_ID in $(aws emr list-clusters --active | jq -r '.Clusters[] | select(.Name | test("^user-batch-analytical-env")) | .Id');
              do
                if [ -z "SINGLE_CLUSTER_ID" ] || [ "SINGLE_CLUSTER_ID" == "$CLUSTER_ID" ]; then
                  aws emr terminate-clusters --cluster-ids $CLUSTER_ID
                fi
              done

    stop-external-batch-cluster:
      task: stop-external-batch-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +x
              for CLUSTER_ID in $(aws emr list-clusters --active | jq -r '.Clusters[] | select(.Name | test("^external-batch-analytical-env")) | .Id');
              do
                aws emr terminate-clusters --cluster-ids $CLUSTER_ID
              done

    stop-waiting-cluster:
      task: stop-waiting-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set -x

              for CLUSTER_ID in `aws emr list-clusters --cluster-state WAITING | jq -r '.Clusters[].Id'`
              do
                  echo "Checking cluster $CLUSTER_ID"

                  NAME_TAG=`aws emr describe-cluster --cluster-id $CLUSTER_ID | jq -r ".Cluster.Tags[] | if .Key == \"Name\" then .Value else empty end"`
                  NAME=`aws emr describe-cluster --cluster-id $CLUSTER_ID | jq -r '.Cluster.Name'`

                  if echo ${NAME} | grep -qE '^batch-analytical-env|^user-batch-analytical-env|^external-batch-analytical-env'; then
                      echo "${NAME} ($CLUSTER_ID) is a batch cluster"
                      END_DATE=`aws emr list-steps --cluster-id $CLUSTER_ID | jq '.Steps[0].Status.Timeline.EndDateTime | floor'`
                      let DELTA="`date +%s` - $END_DATE"

                      if [[ $DELTA -gt 3600 ]]
                      then
                          echo "Stopping cluster ${NAME} ($CLUSTER_ID) with a delta of $DELTA"
                          # aws emr terminate-clusters --cluster-ids $CLUSTER_ID
                      fi
                  fi
              done

    stop-cluster:
      task: stop-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +x

              for CLUSTER_ID in $(aws emr list-clusters --active | jq -r '.Clusters[] | select(.Name | test("aws-analytical-env")) | .Id');
              do
                echo "Stopping cluster with id of ${CLUSTER_ID}"
                aws emr terminate-clusters --cluster-ids $CLUSTER_ID
              done

    clean-security-configs:
      task: clean-security-configs
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +e
              set +x
              SAFE_DATE=$(date +%Y%m%d -d @$(($(date +%s)-172800)))
              echo "SAFE_DATE: $SAFE_DATE"
              for NAME in $(aws emr list-security-configurations | jq -r '.SecurityConfigurations[].Name | match(".*emrfs_20\\d{12}$").string');
              do
                CONFIG_DATE=${NAME:16:8}
                if [ $CONFIG_DATE -lt $SAFE_DATE ]; then
                  echo "Deleting configuration with name of ${NAME}"
                  aws emr delete-security-configuration --name ${NAME}
                fi
              done
              return 0

    monitor-security-configs:
      task: monitor-security-configs
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +e
              set +x

              sc_count=$(aws emr list-security-configurations --query 'length(SecurityConfigurations[])')
              echo "Number of EMR Security Configurations ${sc_count}"
              
              # The maximum number of EMR security configurations you can have is 600. 540 = 90%
              if [ ${sc_count} -gt 540 ]; then
                echo "The number of EMR Security Configurations reaching to maximum quota and need cleaning up!"
                aws sns publish --topic-arn "arn:aws:sns:eu-west-2:$AWS_ACC:Monitoring" --message '{"notification_type": "Error", "severity": "High", "title_text": "EMR Security Configurations above 90% Maximum Threshold"}'
              fi
